{
  "contactInformation": "SHAWN BECKER",
  "positionOrProfessionalTitle": "DATA ENGINEERING / DATA ARCHITECTURE / MACHINE LEARNING",
  "professionalSummary": "As an experienced data engineering professional with expertise in Data Architecture and Machine Learning, I excel in problem-solving and leading teams to deliver innovative solutions to complex challenges across diverse industries, including entertainment, healthcare, finance, and manufacturing. My expertise includes creating scalable, secure, high-volume data pipelines leveraging cloud infrastructure and domain-specific data architecture while employing machine learning models to optimize data-driven decision-making processes. I thrive in dynamic environments, applying Agile methodologies to facilitate my team's focus on building working increments of well-architected solutions that meet product owners' expectations. Driven by a passion for continuous learning, I relish tackling new challenges and achieving excellence in every project.",
  "workExperience": [
    {
      "companyName": "Fannie Mae",
      "location": "Remote",
      "duration": {
        "start": "Mar 2024",
        "end": "Jun 2024"
      },
      "positionOrTitle": "Senior Data Engineer",
      "responsibilities": [
        "Documented processes to build, test, and deploy data pipeline components for in-house ETL framework.",
        "Extensive work with SQL, AWS Redshift, Glue, S3, IAM, Lambda, REST, Postman, SNS, and dbt.",
        "Utilized Agile practices with Jira, including backlog refinements, sprint planning, daily scrums, bi-weekly sprint reviews, and end-of-sprint retrospectives. Enabled the product owner to review each shipped product increment, allowing for potential revision or re-prioritization of backlog items."
      ],
      "skills": [
        "Apache Airflow",
        "Application Migrations",
        "Application Monitoring",
        "AWS CloudWatch",
        "AWS IAM",
        "AWS Lambda",
        "AWS Redshift",
        "AWS S3 Standard",
        "CI/CD",
        "Daily Scrum",
        "Data Catalog",
        "Data Engineering",
        "Databases",
        "Date Partitioning",
        "DevOps",
        "Docker",
        "Docker Compose",
        "ETL",
        "Git",
        "GitHub",
        "Jenkins",
        "Jira",
        "KPI",
        "Linux",
        "Microsoft Excel",
        "Python",
        "Scrum",
        "SDLC",
        "Sprint Planning",
        "Sprint Retrospective",
        "Sprint Review",
        "Sprints",
        "SQL",
        "Unit Testing"
      ]
    },
    {
      "companyName": "The Cigna Group",
      "location": "Remote",
      "duration": {
        "start": "Jun 2023",
        "end": "Dec 2023"
      },
      "positionOrTitle": "Senior Data Engineer",
      "responsibilities": [
        "Modernized apps via Jenkins CI/CD pipeline upgrade, integrating SetupTools, Artifactory/PyPI, SonarQube, and Xray.",
        "Investigated and implemented preparation of legacy ETL data pipeline components. Migration from on-prem Unity IoC apps to the AWS cloud using CDC.",
        "Ensured privacy and encryption standards for user data.",
        "Engineered Python REST API integration enabling credential retrieval from CyberArk's identity management platform using mutual TLS/SSL authentication via AWS API Gateway"
      ],
      "skills": [
        "Apache Airflow",
        "Application Migrations",
        "Application Monitoring",
        "AWS CloudWatch",
        "AWS Glue",
        "AWS IAM",
        "AWS KMS",
        "AWS Lambda",
        "AWS Redshift",
        "AWS S3 Standard",
        "AWS Secrets Manager",
        "AWS Step Functions",
        "Bash",
        "CI/CD",
        "Cyber Security",
        "Daily Scrum",
        "Data Engineering",
        "Databases",
        "DevOps",
        "Docker",
        "Docker Compose",
        "ETL",
        "Git",
        "GitHub",
        "Jenkins",
        "Jira",
        "KPI",
        "Linux",
        "Microsoft Excel",
        "Python",
        "REST APIs",
        "Scrum",
        "SDLC",
        "Sprint Planning",
        "Sprint Retrospective",
        "Sprint Review",
        "Sprints",
        "SQL",
        "Unit Testing"
      ]
    },
    {
      "companyName": "Warner Brothers Interactive Entertainment",
      "location": "Remote",
      "duration": {
        "start": "Sep 2022",
        "end": "Apr 2023"
      },
      "positionOrTitle": "Senior Data Engineer",
      "responsibilities": [
        "Utilized PySpark for ETL processes and Python for third-party integrations and dev-ops collaborations with Jenkins, DataDog, and ZenDesk. I leveraged Google BigQuery and AWS services, including Lambda, Aurora PostgreSQL, SalesForce, Sno, and AWS Glue.",
        "Developed high-volume pipeline ingress and RESTful API integrations, enabling efficient game telemetry and user PII data transfer between WB-distributed games and leading marketing platforms using Segment CDP, Kafka, Redshift, Glue, and Airflow for orchestration.",
        "Conducted exploration and statistical analysis of marketing data, including principal component analysis, eigenvector decomposition, dimensionality reduction, vectorization, Bayesian clustering, and collaborative filtering. I used Jupyter, Python, Pandas, NumPy, Sci-kit Learn, and Keras for analytical processing and Seaborn, Plotly, and Matplotlib for data visualization.",
        "Practiced Agile SDLC in Jira with spring planning, daily scrums, and bi-weekly sprint reviews."
      ]
    },
    {
      "companyName": "Angel Studios",
      "location": "Provo, Utah",
      "duration": {
        "start": "Dec 2021",
        "end": "Aug 2022"
      },
      "positionOrTitle": "Senior Data Engineer",
      "responsibilities": [
        "Used AWS SageMaker, Python, and Machine Learning packages, Pytorch and Keras, to build and optimize a supervised CNN for classifying movie frames from episodes stored in S3.",
        "Earned certifications in Advanced Learning Algorithms, Advanced SQL for Data Scientists, and Supervised Machine Learning: Regression & Classification, boosting professional skills.",
        "Conducted data exploration with machine learning algorithms using Jupyter, Python, Pandas, NumPy, Sci-kit Learn, and Keras for processing, and Seaborn, Plotly, and Matplotlib for data visualization to enhance analytical capabilities significantly.",
        "Created RESTful APIs to exchange data with external e-commerce and advertising partners.",
        "Created Snowflake ingestion scripts to support Looker business intelligence reporting..",
        "Developed effective Business Intelligence strategies using Looker and Tableau with Snowflake and Redshift for comprehensive sales and finance reporting."
      ],
      "skills": [
        "Ansible",
        "Applied Machine Learning",
        "AWS SageMaker",
        "Bash",
        "CI/CD",
        "CNN",
        "Daily Scrum",
        "Data Engineering",
        "Databases",
        "DevOps",
        "ETL",
        "Git",
        "GitHub",
        "Jenkins",
        "Jira",
        "Jupyter",
        "Keras",
        "KPI",
        "Linux",
        "Machine Learning",
        "MatPlotLib",
        "Microsoft Excel",
        "numpy",
        "Pandas",
        "Python",
        "PyTorch",
        "Regularization",
        "Scrum",
        "SDLC",
        "Statistical Data Analysis",
        "Supervised Classification",
        "Supervised Regression",
        "Tableau",
        "Unit Testing"
      ]
    },
    {
      "companyName": "Greenseed Data Laboratory",
      "location": "Orem, Utah",
      "duration": {
        "start": "Nov 2020",
        "end": "Nov 2021"
      },
      "positionOrTitle": "Senior Data Engineer",
      "responsibilities": [
        "Conducted advanced statistical exploration of real-estate sales data using Python, Pandas, NumPy, SciPy, and Scikit-learn.",
        "Implemented a CI/CD pipeline using GitHub Actions with Coverage, SonarQube, and Xray",
        "Designed and built a custom star-schema data warehouse on PostgreSQL, using dimensional modeling, featuring SCD type-2 tables sharing a common streaming facts table.",
        "Managed RESTful APIs used to exchange data with real-estate data teams and customers.",
        "Enhanced machine learning skills with tutorials for TensorFlow, PyTorch, and Keras using Kaggle datasets."
      ],
      "skills": [
        "AWS CloudFormation",
        "AWS CloudWatch",
        "AWS EC2",
        "AWS ECR",
        "AWS ECS",
        "AWS EFS",
        "AWS EKS",
        "AWS Fargate",
        "AWS IAM",
        "AWS Lambda",
        "AWS S3 Standard",
        "Bash",
        "CI/CD",
        "Daily Scrum",
        "Data Cleansing",
        "Data Engineering",
        "Databases",
        "DevOps",
        "Docker",
        "Docker Compose",
        "ETL",
        "Git",
        "GitHub",
        "Jenkins",
        "Jira",
        "KPI",
        "Kubernetes",
        "Linux",
        "Mentoring",
        "Microsoft Excel",
        "PostgreSQL",
        "Python",
        "REST APIs",
        "Scrum",
        "SDLC",
        "SQL",
        "Tech Project Manager",
        "Terraform",
        "Unit Testing"
      ]
    },
    {
      "companyName": "NuSkin",
      "location": "Provo, Utah",
      "duration": {
        "start": "Nov 2019",
        "end": "Nov 2020"
      },
      "positionOrTitle": "Senior Full Stack Developer",
      "responsibilities": [
        "Enhanced site registration and login pages by designing workflow and wireframes.",
        "Innovated Vue Vuetify components with NodeJS SCSS for improved functionality.",
        "Documented and launched new packages for company-wide use, enhancing efficiency.",
        "Internationalized content using Adobe Experience Cloud."
      ],
      "skills": [
        "Bash",
        "CI/CD",
        "Daily Scrum",
        "Data Engineering",
        "Databases",
        "DevOps",
        "Front-End",
        "Git",
        "GitHub",
        "Jenkins",
        "Jira",
        "Linux",
        "Microsoft Excel",
        "Node.js",
        "Plotly",
        "SaaS",
        "Scrum",
        "SDLC",
        "Sprint Planning",
        "Sprint Retrospective",
        "Sprint Review",
        "Sprints",
        "Unit Testing",
        "Vue.js"
      ]
    },
    {
      "companyName": "SeniorLink / Vela",
      "location": "Boston, MA",
      "duration": {
        "start": "Mar 2017",
        "end": "Nov 2019"
      },
      "positionOrTitle": "Senior Data Engineer",
      "responsibilities": [
        "Designed and deployed an AWS data pipeline for the Vela platform, which provided messaging, communication, and collaboration tools for the healthcare industry.",
        "Defined RESTful APIs used by client web applications for posting daily questionnaire forms.",
        "Managed data ingress by queueing API Gateway-delivered message payloads into Amazon Kinesis Data Stream shards. Utilized SNS-triggered Python jobs running on serverless Lambdas to aggregate shard data into date-partitioned Parquet files in an S3 data lake.",
        "for Performed ETL processes, extracting, transforming, and loading Parquet data from the data lake to Redshift via scheduled Databricks PySpark batch jobs on an EMR cluster orchestrated by Data Pipeline.",
        "Ensured privacy and encryption standards for PII, PHI, PCI, Patient Data, FHIR, and HL7, as well as HIPAA and GDPR compliance.",
        "Generated daily business intelligence reports using Tableau with Redshift OLAP.",
        "Coordinated with domestic and offshore development and quality assurance teams."
      ],
      "skills": [
        "Application Monitoring",
        "AWS CloudWatch",
        "AWS DataPipeline",
        "AWS EBS",
        "AWS EC2",
        "AWS ECS",
        "AWS EFS",
        "AWS EMR",
        "AWS IAM",
        "AWS Kinesis",
        "AWS Lambda",
        "AWS RDS",
        "AWS Redshift",
        "AWS S3 Standard",
        "AWS SNS",
        "AWS SQS",
        "Bash",
        "CI/CD",
        "Daily Scrum",
        "Data Architecture",
        "Data Engineering",
        "Data Modeling",
        "Database Design",
        "Databases",
        "Date Partitioning",
        "Denormalization",
        "DevOps",
        "Dimensional Modeling",
        "ETL",
        "Git",
        "GitHub",
        "Java",
        "Jenkins",
        "Jira",
        "Linux",
        "Microsoft Excel",
        "PostgreSQL",
        "PySpark EMR",
        "Python",
        "REST APIs",
        "Schema Evolution",
        "Schema Validation",
        "Scrum",
        "SDLC",
        "Sprint Planning",
        "Sprint Retrospective",
        "Sprint Review",
        "Sprints",
        "Strategic Planning",
        "Tableau",
        "Unit Testing",
        "Visio"
      ]
    },
    {
      "companyName": "ClipFile",
      "location": "Newton Center, MA",
      "duration": {
        "start": "Feb 2011",
        "end": "Mar 2017"
      },
      "positionOrTitle": "Technical Lead, Co-Founder",
      "responsibilities": [
        "Led a team to develop and launch a pioneering SaaS on the AWS platform, empowering individuals and content creators to search and share curated mindsets.",
        "Designed and implemented patented technology, creating a consumer-facing CMS that facilitated fuzzy matching among user-curated quotes and text fragments.",
        "Applied machine learning algorithms, including principal component analysis, eigenvector decomposition, dimensionality reduction, vectorization, cosine similarities, K-means, Bayesian clustering, and collaborative filtering to implement fuzzy word matching and word clustering.",
        "Defined and developed a RESTful API interface for the business logic layer used by the in-app presentation layer and as a service interface for external client apps."
      ],
      "skills": [
        "AWS CloudWatch",
        "AWS DynamoDB",
        "AWS EC2",
        "AWS EFS",
        "AWS Elastic Beanstalk",
        "AWS IAM",
        "AWS Lambda",
        "AWS Load Balancer",
        "AWS Route 53",
        "AWS S3 Standard",
        "AWS SimpleDB",
        "AWS SNS",
        "AWS SQS",
        "AWS VPC",
        "Bash",
        "Bayesian Clustering",
        "Collaborative Filtering",
        "Data Architecture",
        "Data Modeling",
        "Database Design",
        "Databases",
        "DevOps",
        "Dimensionality Reduction",
        "Front-End",
        "Full-Stack",
        "Git",
        "GitHub",
        "JavaScript/HTML/CSS",
        "K-means clustering",
        "Linux",
        "Machine Learning",
        "Mentoring",
        "Microsoft Excel",
        "nltk",
        "Pattern Recognition",
        "REST APIs",
        "SaaS",
        "SDLC",
        "Sentiment Analysis",
        "Spring",
        "Spring Boot",
        "Spring MVC",
        "Sprint Planning",
        "SQL",
        "Strategic Planning",
        "Tech Product Manager",
        "Visio"
      ]
    },
    {
      "companyName": "Sierra Vista Group",
      "location": "Boston, MA",
      "duration": {
        "start": "Nov 2002",
        "end": "Feb 2011"
      },
      "positionOrTitle": "Technical Lead, Co-Founder",
      "responsibilities": [
        "Identified profitable opportunities in product development, software engineering, and data modeling and successfully negotiated budgets and project milestones with C-level management.",
        "Recruited high-value independent consultants specializing in DevOps, full-stack development, database administration, graphic design, user experience, and quality assurance.",
        "Developed and aligned comprehensive project schedules and detailed technical specifications with specific business requirements within strict budgets using the Waterfall SDLC process.",
        "Ensured privacy and encryption standards for PII, PHI, PCI, Patient Data, FHIR, and HL7, as well as HIPAA and GDPR compliance.",
        "Mitigated schedule and budget issues with client management when required.",
        "Managed IT strategies customized for clients in the entertainment, medical services, manufacturing, insurance, and cyber security industries, including AMI, Rowe Jukeboxes, Eleven Systems, Coca-Cola Corp. Europe, Medical Services Corp., and Intrusic Cyber Security."
      ],
      "skills": [
        "AWS CloudWatch",
        "AWS IAM",
        "Bash",
        "Business Requirements",
        "Data Architecture",
        "Data Modeling",
        "Database Design",
        "Databases",
        "DevOps",
        "Front-End",
        "Full-Stack",
        "Git",
        "GitHub",
        "JavaScript/HTML/CSS",
        "Leadership",
        "Linux",
        "Mentoring",
        "Microsoft Excel",
        "Microsoft Project",
        "Presentation Skills",
        "Project Risk",
        "SaaS",
        "SDLC",
        "SQL",
        "Stakeholder Management",
        "Strategic Planning",
        "Tech Product Manager",
        "User Acceptance Testing",
        "Visio"
      ]
    }
  ],
  "education": [
    {
      "institution": "Massachusetts Institute of Technology, Cambridge, Massachusetts,",
      "degree": "PhD, Media Arts & Sciences, Machine Vision/Video Coding"
    },
    {
      "institution": "Brigham Young University, Provo, Utah,",
      "degree": "MS, Computer Science, Medical Imaging/Computer Graphics"
    },
    {
      "institution": "Brigham Young University, Provo, Utah,",
      "degree": "BS, Design Engineering Technology, CAD/CAE/CAM"
    }
  ],
  "skills": "AWS Architecture • Amazon S3 • Amazon EC2 • Amazon VPC • Amazon ElastiCache • Amazon Aurora • Amazon CloudFormation • Docker • Kubernetes • Amazon ECR • Amazon ECS • Amazon EKS • Amazon Fargate • AWS Migration Service • DBT • Amazon Glue • Amazon Glue Catalog • Amazon Lambda • Amazon Step Functions • Kinesis Data Streams • Amazon SQS • Amazon SNS • Amazon Data Pipeline • Amazon EMR • Airflow • Databricks Medallion Architecture • Delta Lake • DataDog • New Relic • Amazon CloudWatch • PostgreSQL • Amazon Redshift • Amazon DynamoDB • Amazon SimpleDB • Oracle • SQL Server  • MongoDB • SQL • Python • PySpark  • Java • Git • REST API • CI/CD • Jenkins • GitHub Actions • GitHub • Bitbucket • Looker • Tableau • Amazon QuickSight • Amazon Sagemaker • Machine Learning • Regression • Classification • CNN • Clustering • Dimensionality Reduction • PCA • RAG • NLP • Encoding • Embedding • Cybersecurity • CyberArk • PII • PHI • PCI • HIPAA • GDPR • Quality Assurance Testing • Patient Data • FHIR • HL7 • Agile Scrum SDLC • MS Project • Visio • Office 365 • Scheduling • Budgets • Milestones • Risk Mitigation • Certified ScrumMaster • Stakeholder Management • Confluence • Jira • Presentation Skills • Communication Skills • Writing Skills • Resource Allocation • Leadership • Tutoring • Team Building • LangChain • NLP • Private GPT • NodeJS • Databricks • Snowflake",
  "publications": "https://independent.academia.edu/shawnbecker",
  "patents": "https://patents.justia.com/inventor/shawn-c-becker",
  "websites": [
    "https://www.linkedin.com/in/shawnbecker",
    "https://github.com/sbecker11",
    "http://spexture.com",
    "https://github.com/sbecker11/flock-of-postcards"
  ],
  "certifications": "https://www.linkedin.com/in/shawnbecker/details/certifications/"
}